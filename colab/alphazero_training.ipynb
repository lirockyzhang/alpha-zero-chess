{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlphaZero Chess Training on Google Colab\n",
    "\n",
    "Train an AlphaZero chess engine using a C++ MCTS backend with GPU-accelerated neural networks.\n",
    "\n",
    "**Requirements:**\n",
    "- Colab GPU runtime (Runtime → Change runtime type → T4 GPU)\n",
    "- Google Drive for persistent checkpoint storage\n",
    "\n",
    "**What this notebook does:**\n",
    "1. Builds the C++ backend (MCTS + self-play engine) from source\n",
    "2. Runs the AlphaZero training loop with parallel self-play\n",
    "3. Saves checkpoints to Google Drive so they persist across sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GPU Check & Google Drive Mount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU is available and mount Google Drive for checkpoint persistence\n",
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\n",
    "        \"No GPU detected! Go to Runtime > Change runtime type > T4 GPU\"\n",
    "    )\n",
    "\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"CUDA: {torch.version.cuda}\")\n",
    "!nvidia-smi --query-gpu=name,memory.total --format=csv,noheader\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "import os\n",
    "DRIVE_DIR = \"/content/drive/MyDrive/alphazero-chess\"\n",
    "os.makedirs(DRIVE_DIR, exist_ok=True)\n",
    "print(f\"\\nDrive output directory: {DRIVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\nREPO_DIR = \"/content/alpha-zero-chess\"\n\nif not os.path.exists(REPO_DIR):\n    !git clone https://github.com/lirockyzhang/alpha-zero-chess.git {REPO_DIR}\nelse:\n    print(f\"Repository already cloned at {REPO_DIR}\")\n    !cd {REPO_DIR} && git pull\n\n# Initialize git submodules (chess-library dependency)\n!cd {REPO_DIR} && git submodule update --init --recursive\n\nos.chdir(REPO_DIR)\nprint(f\"Working directory: {os.getcwd()}\")\n\n# Verify the chess library is present\nchess_hpp = os.path.join(REPO_DIR, \"alphazero-cpp\", \"third_party\", \"chess-library\", \"include\", \"chess.hpp\")\nif os.path.exists(chess_hpp):\n    print(\"chess-library submodule: OK\")\nelse:\n    raise RuntimeError(\n        \"chess-library submodule is missing! \"\n        \"Run: git submodule update --init --recursive\"\n    )"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch and numpy are pre-installed on Colab\n",
    "# We only need pybind11 (for building C++) and python-chess (for the training script)\n",
    "!pip install pybind11 python-chess -q\n",
    "\n",
    "# Verify imports\n",
    "import pybind11, chess, torch, numpy\n",
    "print(f\"pybind11:     {pybind11.__version__}\")\n",
    "print(f\"python-chess: {chess.__version__}\")\n",
    "print(f\"torch:        {torch.__version__}\")\n",
    "print(f\"numpy:        {numpy.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build C++ Module\n",
    "\n",
    "Compiles the C++ MCTS engine and Python bindings. This takes ~2 minutes on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os, subprocess, sys\n\nBUILD_DIR = os.path.join(REPO_DIR, \"alphazero-cpp\", \"build\")\n\n# Clean previous build artifacts to avoid stale cache issues\nif os.path.exists(BUILD_DIR):\n    import shutil\n    shutil.rmtree(BUILD_DIR)\n    print(\"Cleaned previous build directory\")\nos.makedirs(BUILD_DIR)\n\n# Get pybind11 cmake directory\npybind11_dir = subprocess.check_output(\n    [\"python3\", \"-c\", \"import pybind11; print(pybind11.get_cmake_dir())\"]\n).decode().strip()\nprint(f\"pybind11 cmake dir: {pybind11_dir}\")\n\n# Configure - use subprocess to catch errors\nprint(\"\\n--- CMake Configure ---\")\nconfigure_result = subprocess.run(\n    [\n        \"cmake\", \"..\",\n        f\"-DCMAKE_BUILD_TYPE=Release\",\n        f\"-Dpybind11_DIR={pybind11_dir}\",\n    ],\n    cwd=BUILD_DIR,\n    capture_output=True,\n    text=True,\n)\nprint(configure_result.stdout)\nif configure_result.returncode != 0:\n    print(\"STDERR:\", configure_result.stderr)\n    raise RuntimeError(\n        f\"CMake configure failed (exit code {configure_result.returncode}).\\n\"\n        \"Check the output above for details.\"\n    )\n\n# Verify pybind11 was found and Python bindings will be built\nif \"Python bindings will be built\" not in configure_result.stdout:\n    print(\"\\nWARNING: CMake did not find pybind11 or Python!\")\n    print(\"The alphazero_cpp module will NOT be built.\")\n    print(\"CMake output above should show why.\\n\")\n\n# Build using all available cores\nimport multiprocessing\nn_cores = multiprocessing.cpu_count()\nprint(f\"\\n--- CMake Build ({n_cores} cores) ---\")\nbuild_result = subprocess.run(\n    [\"cmake\", \"--build\", \".\", \"--config\", \"Release\", f\"-j{n_cores}\"],\n    cwd=BUILD_DIR,\n    capture_output=True,\n    text=True,\n)\nprint(build_result.stdout[-3000:] if len(build_result.stdout) > 3000 else build_result.stdout)\nif build_result.returncode != 0:\n    print(\"STDERR:\", build_result.stderr[-3000:] if len(build_result.stderr) > 3000 else build_result.stderr)\n    raise RuntimeError(\n        f\"C++ build failed (exit code {build_result.returncode}).\\n\"\n        \"Check the output above for the actual compiler error.\"\n    )\n\n# On Linux, CMake puts the .so directly in build/ (not build/Release/)\n# The training script expects it in build/Release/, so create that structure\nRELEASE_DIR = os.path.join(BUILD_DIR, \"Release\")\nos.makedirs(RELEASE_DIR, exist_ok=True)\n\nimport glob\nso_files = glob.glob(os.path.join(BUILD_DIR, \"alphazero_cpp*.so\"))\nif so_files:\n    for so in so_files:\n        target = os.path.join(RELEASE_DIR, os.path.basename(so))\n        if not os.path.exists(target):\n            os.symlink(so, target)\n            print(f\"Symlinked: {os.path.basename(so)} -> Release/\")\nelse:\n    print(\"\\nERROR: No alphazero_cpp*.so found in build directory!\")\n    print(\"Files in build dir:\", os.listdir(BUILD_DIR))\n    raise RuntimeError(\n        \"Build succeeded but no .so file was produced. \"\n        \"Check if pybind11 was found during CMake configure.\"\n    )\n\n# Verify the module loads\nsys.path.insert(0, RELEASE_DIR)\nsys.path.insert(0, BUILD_DIR)\nimport alphazero_cpp\nprint(f\"\\nalphazero_cpp loaded successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure Training Parameters\n",
    "\n",
    "Adjust these parameters before starting training. The defaults are a good starting point for full training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title Training Configuration { run: \"auto\" }\n\n# --- Network Architecture ---\nFILTERS = 192          # @param {type: \"integer\"}\nBLOCKS = 15            # @param {type: \"integer\"}\n\n# --- Training Loop ---\nITERATIONS = 100       # @param {type: \"integer\"}\nGAMES_PER_ITER = 50    # @param {type: \"integer\"}\nSIMULATIONS = 800      # @param {type: \"integer\"}\nEPOCHS = 5             # @param {type: \"integer\"}\nLR = 0.001             # @param {type: \"number\"}\nTRAIN_BATCH = 256      # @param {type: \"integer\"}\nBUFFER_SIZE = 100000   # @param {type: \"integer\"}\n\n# --- Parallel Self-Play ---\nWORKERS = 64           # @param {type: \"integer\"}\nEVAL_BATCH = 1024      # @param {type: \"integer\"}\nSEARCH_BATCH = 16      # @param {type: \"integer\"}\nGPU_BATCH_TIMEOUT_MS = 10  # @param {type: \"integer\"}\nC_PUCT = 2.0           # @param {type: \"number\"}\n\n# --- Draw Score ---\n# Value assigned to draws from White's perspective.\n# 0.0 = standard symmetric draws.\n# -0.2 = slightly penalize White for drawing (encourages decisive play).\nDRAW_SCORE = 0.0       # @param {type: \"number\"}\n\n# --- Checkpointing ---\nSAVE_INTERVAL = 1      # @param {type: \"integer\"}\n\n# ==============================================================================\n# RESUME OPTIONS (for continuing from a previous checkpoint)\n# ==============================================================================\n# RESUME_RUN_DIR: The run directory to resume from (on Google Drive or local)\n#   - New checkpoints will be saved here\n#   - Leave empty for a fresh start\nRESUME_RUN_DIR = \"\"    # @param {type: \"string\"}\n\n# LOAD_CHECKPOINT_PATH: (Optional) Load model weights from a different location\n#   - Useful for fast loading: upload checkpoint to /content/ instead of reading from Drive\n#   - If empty, loads from RESUME_RUN_DIR (default behavior)\n#   - Example: \"/content/model_iter_010.pt\" (uploaded file)\nLOAD_CHECKPOINT_PATH = \"\"  # @param {type: \"string\"}\n\n# LOAD_REPLAY_BUFFER: Whether to load the replay buffer when resuming\n#   - Buffer files can be 100+ MB and slow to load from Drive\n#   - Set to False to skip loading (training will start with empty buffer)\nLOAD_REPLAY_BUFFER = False  # @param {type: \"boolean\"}\n\n# --- Checkpoint directory setup ---\nimport os\n\nSAVE_DIR = os.path.join(DRIVE_DIR, \"checkpoints\")\nos.makedirs(SAVE_DIR, exist_ok=True)\n\n# Create symlink so the training script's default path also works\nLOCAL_CKPT = os.path.join(REPO_DIR, \"checkpoints\")\nif os.path.islink(LOCAL_CKPT):\n    os.remove(LOCAL_CKPT)\nif not os.path.exists(LOCAL_CKPT):\n    os.symlink(SAVE_DIR, LOCAL_CKPT)\n    print(f\"Symlinked checkpoints/ -> {SAVE_DIR}\")\nelse:\n    print(f\"Note: {LOCAL_CKPT} already exists as a directory, using --save-dir instead\")\n\nprint(f\"\\nCheckpoints will be saved to: {SAVE_DIR}\")\nprint(f\"Network: {FILTERS} filters, {BLOCKS} blocks\")\nprint(f\"Training: {ITERATIONS} iterations, {GAMES_PER_ITER} games/iter, {WORKERS} workers\")\nprint(f\"MCTS: {SIMULATIONS} sims, search_batch={SEARCH_BATCH}, c_puct={C_PUCT}\")\nprint(f\"GPU: eval_batch={EVAL_BATCH}, timeout={GPU_BATCH_TIMEOUT_MS}ms\")\nprint(f\"Draw score: {DRAW_SCORE}\")\nif RESUME_RUN_DIR:\n    print(f\"\\nResuming run directory: {RESUME_RUN_DIR}\")\n    if LOAD_CHECKPOINT_PATH:\n        print(f\"Loading checkpoint from: {LOAD_CHECKPOINT_PATH} (fast local path)\")\n    else:\n        print(f\"Loading checkpoint from: {RESUME_RUN_DIR} (run directory)\")\n    print(f\"Load replay buffer: {LOAD_REPLAY_BUFFER}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. (Optional) Upload Checkpoint for Fast Loading\n\nIf you have a checkpoint file on your computer (e.g., `model_iter_010.pt`), upload it here for **much faster loading** than reading from Google Drive.\n\n**Why this is faster:**\n- Google Drive: ~2-5 MB/s read speed (130MB = 30-60 seconds)\n- Colab local storage: ~500 MB/s (130MB = <1 second)\n\nRun this cell, then set `LOAD_CHECKPOINT_PATH = \"/content/your_model.pt\"` in cell 5."
  },
  {
   "cell_type": "code",
   "source": "# Upload checkpoint file from your computer to Colab's fast local storage\n# After uploading, set LOAD_CHECKPOINT_PATH in cell 5 to the uploaded file path\n\nfrom google.colab import files\nimport os\n\nprint(\"Select a checkpoint file (.pt) to upload...\")\nprint(\"(This will be stored in /content/ for fast access)\\n\")\n\nuploaded = files.upload()\n\nif uploaded:\n    for filename in uploaded.keys():\n        filepath = f\"/content/{filename}\"\n        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n        print(f\"\\n✓ Uploaded: {filepath} ({size_mb:.1f} MB)\")\n        print(f\"\\nTo use this checkpoint, set in cell 5:\")\n        print(f'  LOAD_CHECKPOINT_PATH = \"{filepath}\"')\nelse:\n    print(\"No file uploaded.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Run Training\n\nThis starts the AlphaZero training loop. Progress is printed every 30 seconds.\n\nCheckpoints are saved to Google Drive every `SAVE_INTERVAL` iterations, so they persist even if Colab disconnects.\n\n**First run will be slower** due to:\n- `torch.compile()` JIT compilation (~2-5 minutes, one-time)\n- cuDNN algorithm auto-tuning (first few batches)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\nimport shutil\n\n# Handle checkpoint loading from a different location\nif RESUME_RUN_DIR and LOAD_CHECKPOINT_PATH:\n    # User wants to load from a fast local path but save to the run directory\n    # Copy the checkpoint to the run directory so --resume works correctly\n    if os.path.exists(LOAD_CHECKPOINT_PATH):\n        # Determine the target filename\n        ckpt_filename = os.path.basename(LOAD_CHECKPOINT_PATH)\n        target_path = os.path.join(RESUME_RUN_DIR, ckpt_filename)\n        \n        # Copy if not already there (or if source is newer)\n        if not os.path.exists(target_path) or \\\n           os.path.getmtime(LOAD_CHECKPOINT_PATH) > os.path.getmtime(target_path):\n            print(f\"Copying checkpoint to run directory...\")\n            print(f\"  From: {LOAD_CHECKPOINT_PATH}\")\n            print(f\"  To:   {target_path}\")\n            shutil.copy2(LOAD_CHECKPOINT_PATH, target_path)\n            print(f\"  Done! ({os.path.getsize(target_path) / 1024 / 1024:.1f} MB)\")\n        else:\n            print(f\"Checkpoint already in run directory: {target_path}\")\n    else:\n        raise FileNotFoundError(f\"Checkpoint not found: {LOAD_CHECKPOINT_PATH}\")\n\n# Build the training command\ncmd = (\n    f\"python {os.path.join(REPO_DIR, 'alphazero-cpp', 'scripts', 'train.py')}\"\n    f\" --iterations {ITERATIONS}\"\n    f\" --games-per-iter {GAMES_PER_ITER}\"\n    f\" --simulations {SIMULATIONS}\"\n    f\" --workers {WORKERS}\"\n    f\" --eval-batch {EVAL_BATCH}\"\n    f\" --search-batch {SEARCH_BATCH}\"\n    f\" --gpu-batch-timeout-ms {GPU_BATCH_TIMEOUT_MS}\"\n    f\" --c-puct {C_PUCT}\"\n    f\" --filters {FILTERS}\"\n    f\" --blocks {BLOCKS}\"\n    f\" --train-batch {TRAIN_BATCH}\"\n    f\" --lr {LR}\"\n    f\" --epochs {EPOCHS}\"\n    f\" --buffer-size {BUFFER_SIZE}\"\n    f\" --save-dir {SAVE_DIR}\"\n    f\" --save-interval {SAVE_INTERVAL}\"\n    f\" --progress-interval 30\"\n    f\" --no-visualization\"\n    f\" --device cuda\"\n    f\" --draw-score {DRAW_SCORE}\"\n)\n\n# Add resume options\nif RESUME_RUN_DIR:\n    cmd += f\" --resume {RESUME_RUN_DIR}\"\n    if LOAD_REPLAY_BUFFER:\n        cmd += \" --load-buffer\"\n\nprint(\"Training command:\")\nprint(cmd)\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Starting training...\")\nprint(\"=\" * 60 + \"\\n\")\n\n!{cmd}",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os, json\n\nprint(\"=\" * 60)\nprint(\"Saved checkpoints on Google Drive:\")\nprint(\"=\" * 60)\n\nif os.path.exists(SAVE_DIR):\n    for run_dir in sorted(os.listdir(SAVE_DIR)):\n        run_path = os.path.join(SAVE_DIR, run_dir)\n        if os.path.isdir(run_path):\n            files = os.listdir(run_path)\n            pt_files = sorted([f for f in files if f.endswith(\".pt\") and not f.endswith(\"_emergency.pt\")])\n            print(f\"\\n  {run_dir}/\")\n            for f in pt_files[-5:]:  # Show last 5 checkpoints\n                size_mb = os.path.getsize(os.path.join(run_path, f)) / (1024 * 1024)\n                print(f\"    {f}  ({size_mb:.1f} MB)\")\n            if len(pt_files) > 5:\n                print(f\"    ... and {len(pt_files) - 5} more checkpoints\")\n\n            # Show training metrics if available\n            metrics_path = os.path.join(run_path, \"metrics_history.json\")\n            if os.path.exists(metrics_path):\n                with open(metrics_path) as mf:\n                    metrics = json.load(mf)\n                iters = metrics.get(\"iterations\", [])\n                if iters:\n                    print(f\"    Iterations completed: {len(iters)}\")\n                    last = iters[-1]\n                    print(f\"    Latest loss: {last.get('loss', 'N/A'):.4f}\")\nelse:\n    print(\"  No checkpoints found.\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"To resume training in a new session:\")\nprint(\"=\" * 60)\nprint(f\"\"\"\n  1. Run cells 1-5 (setup)\n  \n  2. In cell 5 (Configuration), set:\n     RESUME_RUN_DIR = \"{SAVE_DIR}/<run_directory>\"\n  \n  3. (Optional) For FAST checkpoint loading:\n     - Run cell 6 to upload checkpoint from your computer\n     - Set: LOAD_CHECKPOINT_PATH = \"/content/model_iter_XXX.pt\"\n     - Set: LOAD_REPLAY_BUFFER = False  (skip slow buffer loading)\n  \n  4. Run cell 7 to continue training\n\"\"\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Saved checkpoints on Google Drive:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if os.path.exists(SAVE_DIR):\n",
    "    for run_dir in sorted(os.listdir(SAVE_DIR)):\n",
    "        run_path = os.path.join(SAVE_DIR, run_dir)\n",
    "        if os.path.isdir(run_path):\n",
    "            files = os.listdir(run_path)\n",
    "            pt_files = sorted([f for f in files if f.endswith(\".pt\")])\n",
    "            print(f\"\\n  {run_dir}/\")\n",
    "            for f in pt_files:\n",
    "                size_mb = os.path.getsize(os.path.join(run_path, f)) / (1024 * 1024)\n",
    "                print(f\"    {f}  ({size_mb:.1f} MB)\")\n",
    "\n",
    "            # Show training metrics if available\n",
    "            metrics_path = os.path.join(run_path, \"training_metrics.json\")\n",
    "            if os.path.exists(metrics_path):\n",
    "                with open(metrics_path) as mf:\n",
    "                    metrics = json.load(mf)\n",
    "                n_iters = len(metrics)\n",
    "                if n_iters > 0:\n",
    "                    last = metrics[-1]\n",
    "                    print(f\"    Iterations completed: {n_iters}\")\n",
    "                    print(f\"    Final loss: {last.get('total_loss', 'N/A')}\")\n",
    "else:\n",
    "    print(\"  No checkpoints found.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"To resume training in a new session:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  1. Run cells 1-5 again\")\n",
    "print(f\"  2. Set RESUME_PATH to the run directory, e.g.:\")\n",
    "print(f\"     RESUME_PATH = \\\"{SAVE_DIR}/<run_directory>\\\"\")\n",
    "print(f\"  3. Run cell 6 to continue training\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}