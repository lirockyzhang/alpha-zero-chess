{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlphaZero Chess Training on Google Colab\n",
    "\n",
    "Train an AlphaZero chess engine using a C++ MCTS backend with GPU-accelerated neural networks.\n",
    "\n",
    "**Requirements:**\n",
    "- Colab GPU runtime (Runtime → Change runtime type → T4 GPU)\n",
    "- Google Drive for persistent checkpoint storage\n",
    "\n",
    "**What this notebook does:**\n",
    "1. Builds the C++ backend (MCTS + self-play engine) from source\n",
    "2. Runs the AlphaZero training loop with parallel self-play\n",
    "3. Saves checkpoints to Google Drive so they persist across sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GPU Check & Google Drive Mount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU is available and mount Google Drive for checkpoint persistence\n",
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\n",
    "        \"No GPU detected! Go to Runtime > Change runtime type > T4 GPU\"\n",
    "    )\n",
    "\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"CUDA: {torch.version.cuda}\")\n",
    "!nvidia-smi --query-gpu=name,memory.total --format=csv,noheader\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "import os\n",
    "DRIVE_DIR = \"/content/drive/MyDrive/alphazero-chess\"\n",
    "os.makedirs(DRIVE_DIR, exist_ok=True)\n",
    "print(f\"\\nDrive output directory: {DRIVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\nREPO_DIR = \"/content/alpha-zero-chess\"\n\nif not os.path.exists(REPO_DIR):\n    !git clone https://github.com/lirockyzhang/alpha-zero-chess.git {REPO_DIR}\nelse:\n    print(f\"Repository already cloned at {REPO_DIR}\")\n    !cd {REPO_DIR} && git pull\n\n# Initialize git submodules (chess-library dependency)\n!cd {REPO_DIR} && git submodule update --init --recursive\n\nos.chdir(REPO_DIR)\nprint(f\"Working directory: {os.getcwd()}\")\n\n# Verify the chess library is present\nchess_hpp = os.path.join(REPO_DIR, \"alphazero-cpp\", \"third_party\", \"chess-library\", \"include\", \"chess.hpp\")\nif os.path.exists(chess_hpp):\n    print(\"chess-library submodule: OK\")\nelse:\n    raise RuntimeError(\n        \"chess-library submodule is missing! \"\n        \"Run: git submodule update --init --recursive\"\n    )"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch and numpy are pre-installed on Colab\n",
    "# We only need pybind11 (for building C++) and python-chess (for the training script)\n",
    "!pip install pybind11 python-chess -q\n",
    "\n",
    "# Verify imports\n",
    "import pybind11, chess, torch, numpy\n",
    "print(f\"pybind11:     {pybind11.__version__}\")\n",
    "print(f\"python-chess: {chess.__version__}\")\n",
    "print(f\"torch:        {torch.__version__}\")\n",
    "print(f\"numpy:        {numpy.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build C++ Module\n",
    "\n",
    "Compiles the C++ MCTS engine and Python bindings. This takes ~2 minutes on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os, subprocess, sys\n\nBUILD_DIR = os.path.join(REPO_DIR, \"alphazero-cpp\", \"build\")\n\n# Clean previous build artifacts to avoid stale cache issues\nif os.path.exists(BUILD_DIR):\n    import shutil\n    shutil.rmtree(BUILD_DIR)\n    print(\"Cleaned previous build directory\")\nos.makedirs(BUILD_DIR)\n\n# Get pybind11 cmake directory\npybind11_dir = subprocess.check_output(\n    [\"python3\", \"-c\", \"import pybind11; print(pybind11.get_cmake_dir())\"]\n).decode().strip()\nprint(f\"pybind11 cmake dir: {pybind11_dir}\")\n\n# Configure - use subprocess to catch errors\nprint(\"\\n--- CMake Configure ---\")\nconfigure_result = subprocess.run(\n    [\n        \"cmake\", \"..\",\n        f\"-DCMAKE_BUILD_TYPE=Release\",\n        f\"-Dpybind11_DIR={pybind11_dir}\",\n    ],\n    cwd=BUILD_DIR,\n    capture_output=True,\n    text=True,\n)\nprint(configure_result.stdout)\nif configure_result.returncode != 0:\n    print(\"STDERR:\", configure_result.stderr)\n    raise RuntimeError(\n        f\"CMake configure failed (exit code {configure_result.returncode}).\\n\"\n        \"Check the output above for details.\"\n    )\n\n# Verify pybind11 was found and Python bindings will be built\nif \"Python bindings will be built\" not in configure_result.stdout:\n    print(\"\\nWARNING: CMake did not find pybind11 or Python!\")\n    print(\"The alphazero_cpp module will NOT be built.\")\n    print(\"CMake output above should show why.\\n\")\n\n# Build using all available cores\nimport multiprocessing\nn_cores = multiprocessing.cpu_count()\nprint(f\"\\n--- CMake Build ({n_cores} cores) ---\")\nbuild_result = subprocess.run(\n    [\"cmake\", \"--build\", \".\", \"--config\", \"Release\", f\"-j{n_cores}\"],\n    cwd=BUILD_DIR,\n    capture_output=True,\n    text=True,\n)\nprint(build_result.stdout[-3000:] if len(build_result.stdout) > 3000 else build_result.stdout)\nif build_result.returncode != 0:\n    print(\"STDERR:\", build_result.stderr[-3000:] if len(build_result.stderr) > 3000 else build_result.stderr)\n    raise RuntimeError(\n        f\"C++ build failed (exit code {build_result.returncode}).\\n\"\n        \"Check the output above for the actual compiler error.\"\n    )\n\n# On Linux, CMake puts the .so directly in build/ (not build/Release/)\n# The training script expects it in build/Release/, so create that structure\nRELEASE_DIR = os.path.join(BUILD_DIR, \"Release\")\nos.makedirs(RELEASE_DIR, exist_ok=True)\n\nimport glob\nso_files = glob.glob(os.path.join(BUILD_DIR, \"alphazero_cpp*.so\"))\nif so_files:\n    for so in so_files:\n        target = os.path.join(RELEASE_DIR, os.path.basename(so))\n        if not os.path.exists(target):\n            os.symlink(so, target)\n            print(f\"Symlinked: {os.path.basename(so)} -> Release/\")\nelse:\n    print(\"\\nERROR: No alphazero_cpp*.so found in build directory!\")\n    print(\"Files in build dir:\", os.listdir(BUILD_DIR))\n    raise RuntimeError(\n        \"Build succeeded but no .so file was produced. \"\n        \"Check if pybind11 was found during CMake configure.\"\n    )\n\n# Verify the module loads\nsys.path.insert(0, RELEASE_DIR)\nsys.path.insert(0, BUILD_DIR)\nimport alphazero_cpp\nprint(f\"\\nalphazero_cpp loaded successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure Training Parameters\n",
    "\n",
    "Adjust these parameters before starting training. The defaults are a good starting point for full training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title Training Configuration { run: \"auto\" }\n\n# --- Network Architecture ---\nFILTERS = 192          # @param {type: \"integer\"}\nBLOCKS = 15            # @param {type: \"integer\"}\n\n# --- Training Loop ---\nITERATIONS = 100       # @param {type: \"integer\"}\nGAMES_PER_ITER = 50    # @param {type: \"integer\"}\nSIMULATIONS = 800      # @param {type: \"integer\"}\nEPOCHS = 5             # @param {type: \"integer\"}\nLR = 0.001             # @param {type: \"number\"}\nTRAIN_BATCH = 256      # @param {type: \"integer\"}\nBUFFER_SIZE = 100000   # @param {type: \"integer\"}\n\n# --- Parallel Self-Play ---\nWORKERS = 16           # @param {type: \"integer\"}\nEVAL_BATCH = 512       # @param {type: \"integer\"}\nGPU_BATCH_TIMEOUT_MS = 20  # @param {type: \"integer\"}\n\n# --- Draw Score ---\n# Value assigned to draws from White's perspective.\n# 0.0 = standard symmetric draws.\n# -0.5 = penalize White for drawing (encourages decisive play in early training).\nDRAW_SCORE = 0.0       # @param {type: \"number\"}\n\n# --- Checkpointing ---\nSAVE_INTERVAL = 5      # @param {type: \"integer\"}\n\n# --- Resume from previous run (leave empty for fresh start) ---\nRESUME_PATH = \"\"       # @param {type: \"string\"}\n\n# --- Checkpoint directory setup ---\nimport os\n\nSAVE_DIR = os.path.join(DRIVE_DIR, \"checkpoints\")\nos.makedirs(SAVE_DIR, exist_ok=True)\n\n# Create symlink so the training script's default path also works\nLOCAL_CKPT = os.path.join(REPO_DIR, \"checkpoints\")\nif os.path.islink(LOCAL_CKPT):\n    os.remove(LOCAL_CKPT)\nif not os.path.exists(LOCAL_CKPT):\n    os.symlink(SAVE_DIR, LOCAL_CKPT)\n    print(f\"Symlinked checkpoints/ -> {SAVE_DIR}\")\nelse:\n    print(f\"Note: {LOCAL_CKPT} already exists as a directory, using --save-dir instead\")\n\nprint(f\"\\nCheckpoints will be saved to: {SAVE_DIR}\")\nprint(f\"Network: {FILTERS} filters, {BLOCKS} blocks\")\nprint(f\"Training: {ITERATIONS} iterations, {GAMES_PER_ITER} games/iter, {WORKERS} workers\")\nprint(f\"GPU batch timeout: {GPU_BATCH_TIMEOUT_MS}ms\")\nprint(f\"Draw score: {DRAW_SCORE}\")\nif RESUME_PATH:\n    print(f\"Resuming from: {RESUME_PATH}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Training\n",
    "\n",
    "This starts the AlphaZero training loop. Progress is printed every 30 seconds.\n",
    "\n",
    "Checkpoints are saved to Google Drive every `SAVE_INTERVAL` iterations, so they persist even if Colab disconnects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\ncmd = (\n    f\"python {os.path.join(REPO_DIR, 'alphazero-cpp', 'scripts', 'train.py')}\"\n    f\" --iterations {ITERATIONS}\"\n    f\" --games-per-iter {GAMES_PER_ITER}\"\n    f\" --simulations {SIMULATIONS}\"\n    f\" --workers {WORKERS}\"\n    f\" --eval-batch {EVAL_BATCH}\"\n    f\" --gpu-batch-timeout-ms {GPU_BATCH_TIMEOUT_MS}\"\n    f\" --filters {FILTERS}\"\n    f\" --blocks {BLOCKS}\"\n    f\" --train-batch {TRAIN_BATCH}\"\n    f\" --lr {LR}\"\n    f\" --epochs {EPOCHS}\"\n    f\" --buffer-size {BUFFER_SIZE}\"\n    f\" --save-dir {SAVE_DIR}\"\n    f\" --save-interval {SAVE_INTERVAL}\"\n    f\" --progress-interval 30\"\n    f\" --no-visualization\"\n    f\" --no-eval\"\n    f\" --device cuda\"\n    f\" --draw-score {DRAW_SCORE}\"\n)\n\nif RESUME_PATH:\n    cmd += f\" --resume {RESUME_PATH}\"\n\nprint(\"Training command:\")\nprint(cmd)\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Starting training...\")\nprint(\"=\" * 60 + \"\\n\")\n\n!{cmd}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Saved checkpoints on Google Drive:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if os.path.exists(SAVE_DIR):\n",
    "    for run_dir in sorted(os.listdir(SAVE_DIR)):\n",
    "        run_path = os.path.join(SAVE_DIR, run_dir)\n",
    "        if os.path.isdir(run_path):\n",
    "            files = os.listdir(run_path)\n",
    "            pt_files = sorted([f for f in files if f.endswith(\".pt\")])\n",
    "            print(f\"\\n  {run_dir}/\")\n",
    "            for f in pt_files:\n",
    "                size_mb = os.path.getsize(os.path.join(run_path, f)) / (1024 * 1024)\n",
    "                print(f\"    {f}  ({size_mb:.1f} MB)\")\n",
    "\n",
    "            # Show training metrics if available\n",
    "            metrics_path = os.path.join(run_path, \"training_metrics.json\")\n",
    "            if os.path.exists(metrics_path):\n",
    "                with open(metrics_path) as mf:\n",
    "                    metrics = json.load(mf)\n",
    "                n_iters = len(metrics)\n",
    "                if n_iters > 0:\n",
    "                    last = metrics[-1]\n",
    "                    print(f\"    Iterations completed: {n_iters}\")\n",
    "                    print(f\"    Final loss: {last.get('total_loss', 'N/A')}\")\n",
    "else:\n",
    "    print(\"  No checkpoints found.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"To resume training in a new session:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  1. Run cells 1-5 again\")\n",
    "print(f\"  2. Set RESUME_PATH to the run directory, e.g.:\")\n",
    "print(f\"     RESUME_PATH = \\\"{SAVE_DIR}/<run_directory>\\\"\")\n",
    "print(f\"  3. Run cell 6 to continue training\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}